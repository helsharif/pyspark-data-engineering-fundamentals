{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d910a63e",
   "metadata": {},
   "source": [
    "# Intro to Spark\n",
    "## Husayn El Sharif\n",
    "## Refer to: https://academy.zerotomastery.io/courses/data-engineering-bootcamp/lectures/60978439\n",
    "### Use environment spark4_env001 (see spark4_env001.yml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "254953c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from pyspark.sql import SparkSession # Import SparkSession from PySpark. SQL module\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType # Import data types for defining schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "194bea26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/25 08:52:22 WARN Utils: Your hostname, Husayn-SLS2, resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/12/25 08:52:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/25 08:52:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session instance\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master(\"local[*]\")  # Use local and all available cores\n",
    "    .appName(\"MyFirstSparkApp\")  # Name of the application\n",
    "    .getOrCreate()  # Create the Spark session (or get it if it already exists)\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765a9732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Apache Spark version is: 4.1.0\n"
     ]
    }
   ],
   "source": [
    "# Print the Spark version\n",
    "print(f\"The Apache Spark version is: {spark.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "628a12af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define schema instead of inferring it\n",
    "schema = StructType([\n",
    "    StructField(\"step\", IntegerType(), True), # True means the field can be null\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"nameOrig\", StringType(), True),\n",
    "    StructField(\"oldbalanceOrg\", DoubleType(), True),\n",
    "    StructField(\"newbalanceOrig\", DoubleType(), True),\n",
    "    StructField(\"nameDest\", StringType(), True),\n",
    "    StructField(\"oldbalanceDest\", DoubleType(), True),\n",
    "    StructField(\"newbalanceDest\", DoubleType(), True),\n",
    "    StructField(\"isFraud\", IntegerType(), True),\n",
    "    StructField(\"isFlaggedFraud\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce10453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read a CSV file into a DataFrame\n",
    "df_inferred_schema = spark.read.csv(\"example_csv_data/example_data.csv\", \n",
    "                    header=True, # Use first row as header\n",
    "                    inferSchema=True, # Infer data types from data, can be slow for large datasets\n",
    "                    quote='\"',  # Use double quotes as quote character\n",
    "                    mode=\"PERMISSIVE\" # Handle malformed lines by setting fields to null (other options: DROPMALFORMED which drops malformed lines, FAILFAST which throws an error)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "066fc271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a CSV file into a DataFrame with defined schema, much faster than inferring schema\n",
    "df_defined_schema = spark.read.csv(\"example_csv_data/example_data.csv\", \n",
    "                    header=True, # Use first row as header\n",
    "                    schema=schema, # Use the defined schema. If data doesn't match schema, errors will be null\n",
    "                    quote='\"',  # Use double quotes as quote character\n",
    "                    mode=\"PERMISSIVE\" # Handle malformed lines by setting fields to null (other options: DROPMALFORMED which drops malformed lines, FAILFAST which throws an error)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b82b4c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048575"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helpful methods\n",
    "df_defined_schema.count()  # Get number of rows in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ca54056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|step|    type|  amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|   1| PAYMENT| 9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT| 1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|             0|\n",
      "|   1|TRANSFER|   181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT|   181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|      1|             0|\n",
      "|   1| PAYMENT|11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT| 7817.71|  C90045638|      53860.0|      46042.29| M573487274|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT| 7107.77| C154988899|     183195.0|     176087.23| M408069119|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT| 7861.64|C1912850431|    176087.23|     168225.59| M633326333|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT| 4024.36|C1265012928|       2671.0|           0.0|M1176932104|           0.0|           0.0|      0|             0|\n",
      "|   1|   DEBIT| 5337.77| C712410124|      41720.0|      36382.23| C195600860|       41898.0|      40348.79|      0|             0|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "df_defined_schema.show(10)  # Show first 10 rows of DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "929c28bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+\n",
      "|step|    type|  amount|\n",
      "+----+--------+--------+\n",
      "|   1| PAYMENT| 9839.64|\n",
      "|   1| PAYMENT| 1864.28|\n",
      "|   1|TRANSFER|   181.0|\n",
      "|   1|CASH_OUT|   181.0|\n",
      "|   1| PAYMENT|11668.14|\n",
      "+----+--------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Select specific columns\n",
    "df2 = df_defined_schema.select(\"step\", \"type\", \"amount\")    \n",
    "df2.show(5)  # Show first 5 rows of the new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "022fe550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------------+\n",
      "|step|    type|amount_cents|\n",
      "+----+--------+------------+\n",
      "|   1| PAYMENT|    983964.0|\n",
      "|   1| PAYMENT|    186428.0|\n",
      "|   1|TRANSFER|     18100.0|\n",
      "|   1|CASH_OUT|     18100.0|\n",
      "|   1| PAYMENT|   1166814.0|\n",
      "+----+--------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df3 = df_defined_schema.select(\n",
    "    df_defined_schema[\"step\"],\n",
    "    df_defined_schema[\"type\"],\n",
    "    (df_defined_schema[\"amount\"]*100).alias(\"amount_cents\")  # Create a new column with amount in cents\n",
    ")\n",
    "\n",
    "df3.show(5)  # Show first 5 rows of the new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d03aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the Spark session\n",
    "spark.stop() # Stop the Spark session when done, freeing up resources, especially important in production environments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark4_env001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
