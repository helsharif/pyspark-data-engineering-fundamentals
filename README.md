# PySpark Fundamentals & Practical Exercises

Learning-focused repository covering the **core foundations of PySpark**, with practical, industry-relevant exercises that reflect how data scientists and data engineers actually use Spark in real workflows.

This repo demonstrates hands-on experience with distributed data processing, Spark DataFrames, ETL, performance considerations, and applied analytics at scale.

Based on Zero-to-Mastery's Data Engineering BootCamp learning modules at: https://zerotomastery.io/courses/data-engineering-bootcamp/

---

## ğŸ¯ Objectives

By the end of this learning repo, I aimed to:
- Understand Spark architecture & execution model
- Work confidently with **Spark DataFrames**
- Perform **ETL-style transformations** at scale
- Write **efficient, productionâ€‘minded PySpark code**
- Explore **basic ML & analytics workflows** with Spark
- Practice working with real-world style structured data

---

## ğŸ§± Topics Covered

âœ”ï¸ Spark Sessions & Lazy Execution  
âœ”ï¸ DataFrames: creation, schema, typing  
âœ”ï¸ Reading / writing data (CSV, Parquet)  
âœ”ï¸ Filtering, joins, aggregations  
âœ”ï¸ Handling nulls & data cleaning  
âœ”ï¸ UDFs vs native Spark functions  
âœ”ï¸ Performance awareness & best practices  
âœ”ï¸ Intro to Spark ML 

---

## ğŸ—‚ï¸ Repository Structure

```
pyspark-data-engineering-fundamentals/
â”‚
â”œâ”€â”€ example_csv_data/ # Practice datasets used in notebooks
â”‚
â”œâ”€â”€ ztm-data-engineering-main/ # Course / reference material folder
â”‚
â”œâ”€â”€ e01_pyspark_intro.ipynb # Intro notebook (Spark basics)
â”‚
â”œâ”€â”€ .gitignore # Ignore environment + build files
â”‚
â”œâ”€â”€ spark_env001.yml - Shortcut.lnk # Shortcut to conda environment file
â”‚
â””â”€â”€ README.md # Project overview
```

> Note: Notebooks intentionally emphasize clarity, explanation, and applied reasoning â€” not just code.

---

## âš™ï¸ Environment & Setup

This project was built using **Python 3.11 + PySpark** in a Conda environment.

Example setup:

```bash
conda create -n spark_env python=3.11
conda activate spark_env
pip install pyspark
pip install pandas numpy
```

Then start Jupyter:

```bash
jupyter lab
```

---

## â–¶ï¸ Running the Notebooks

1ï¸âƒ£ Clone repository  
2ï¸âƒ£ Start your Spark-enabled environment  
3ï¸âƒ£ Open `/notebooks` in Jupyter / VS Code  
4ï¸âƒ£ Run cells sequentially

---

## ğŸ“Œ Why This Repo Exists

Hiring teams increasingly expect:
- Experience working with distributed compute tools
- Comfort with **big data workflows**
- Ability to build reliable pipelines â€” not just notebooks

This repo demonstrates **hands-on Spark capability**, problemâ€‘solving, and familiarity with scalable analytics tools.

---

## ğŸ§  Notes & Learning Mindset

This is not meant to be â€œperfectâ€; it is a **learning lab**.  
Iâ€™ve intentionally included explanations, comments, and intermediate thinking throughout the notebooks.

---

## ğŸŒŸ Next Steps (Future Enhancements)

- Larger real dataset exploration (e.g., NYC Taxi, weather, etc.)
- Spark SQL
- More Spark ML examples
- Structured streaming exploration

---

## ğŸ¤ Feedback Welcome

If you're a recruiter, data team lead, or engineer reviewing this â€” Iâ€™d love feedback!  
Always improving, always learning.

---

**Author:** Husayn El Sharif  
